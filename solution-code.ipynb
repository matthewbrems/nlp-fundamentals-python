{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This iiiiiis Jeopardyyy!\n",
    "\n",
    "> [Source of Dataset](https://www.reddit.com/r/datasets/comments/1uyd0t/200000_jeopardy_questions_in_a_json_file/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages.\n",
    "# !pip3 install pandas regex sklearn --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agenda\n",
    "\n",
    "1. Import and explore our data using `pandas`.\n",
    "2. Clean our text data.\n",
    "3. Vectorize our text data.\n",
    "4. Fit model.\n",
    "5. Evaluate performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and explore our data using `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read our Jeopardy data in.\n",
    "jeopardy = pd.read_csv('./jeopardy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>show_number</th>\n",
       "      <th>air_date</th>\n",
       "      <th>round</th>\n",
       "      <th>category</th>\n",
       "      <th>value</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4680</td>\n",
       "      <td>12/31/04</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>HISTORY</td>\n",
       "      <td>$200</td>\n",
       "      <td>For the last 8 years of his life, Galileo was ...</td>\n",
       "      <td>Copernicus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4680</td>\n",
       "      <td>12/31/04</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>ESPN's TOP 10 ALL-TIME ATHLETES</td>\n",
       "      <td>$200</td>\n",
       "      <td>No. 2: 1912 Olympian; football star at Carlisl...</td>\n",
       "      <td>Jim Thorpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4680</td>\n",
       "      <td>12/31/04</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EVERYBODY TALKS ABOUT IT...</td>\n",
       "      <td>$200</td>\n",
       "      <td>The city of Yuma in this state has a record av...</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4680</td>\n",
       "      <td>12/31/04</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>THE COMPANY LINE</td>\n",
       "      <td>$200</td>\n",
       "      <td>In 1963, live on \"The Art Linkletter Show\", th...</td>\n",
       "      <td>McDonald's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4680</td>\n",
       "      <td>12/31/04</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EPITAPHS &amp; TRIBUTES</td>\n",
       "      <td>$200</td>\n",
       "      <td>Signer of the Dec. of Indep., framer of the Co...</td>\n",
       "      <td>John Adams</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   show_number  air_date      round                         category  value  \\\n",
       "0         4680  12/31/04  Jeopardy!                          HISTORY  $200    \n",
       "1         4680  12/31/04  Jeopardy!  ESPN's TOP 10 ALL-TIME ATHLETES  $200    \n",
       "2         4680  12/31/04  Jeopardy!      EVERYBODY TALKS ABOUT IT...  $200    \n",
       "3         4680  12/31/04  Jeopardy!                 THE COMPANY LINE  $200    \n",
       "4         4680  12/31/04  Jeopardy!              EPITAPHS & TRIBUTES  $200    \n",
       "\n",
       "                                            question      answer  \n",
       "0  For the last 8 years of his life, Galileo was ...  Copernicus  \n",
       "1  No. 2: 1912 Olympian; football star at Carlisl...  Jim Thorpe  \n",
       "2  The city of Yuma in this state has a record av...     Arizona  \n",
       "3  In 1963, live on \"The Art Linkletter Show\", th...  McDonald's  \n",
       "4  Signer of the Dec. of Indep., framer of the Co...  John Adams  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the first five rows.\n",
    "jeopardy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert air_date to a datetime column.\n",
    "jeopardy['air_date'] = pd.to_datetime(jeopardy['air_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2012-01-27 00:00:00')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the latest air_date?\n",
    "max(jeopardy['air_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('1984-09-10 00:00:00')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the earliest air_date?\n",
    "min(jeopardy['air_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Jeopardy!           107384\n",
       "Double Jeopardy!    105912\n",
       "Final Jeopardy!       3631\n",
       "Tiebreaker               3\n",
       "Name: round, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the breakdown of questions by round?\n",
    "jeopardy['round'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BEFORE & AFTER              547\n",
       "SCIENCE                     519\n",
       "LITERATURE                  496\n",
       "AMERICAN HISTORY            418\n",
       "POTPOURRI                   401\n",
       "                           ... \n",
       "LITERATURE OF THE 1800s       1\n",
       "FICTIONAL PEOPLE              1\n",
       "19th CENTURY POLITICIANS      1\n",
       "WOMEN IN POEMS                1\n",
       "SEMIANNUAL PUBLICATIONS       1\n",
       "Name: category, Length: 27983, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the breakdown of questions by category?\n",
    "jeopardy['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>show_number</th>\n",
       "      <th>air_date</th>\n",
       "      <th>round</th>\n",
       "      <th>category</th>\n",
       "      <th>value</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12305</th>\n",
       "      <td>5332</td>\n",
       "      <td>2007-11-13</td>\n",
       "      <td>Tiebreaker</td>\n",
       "      <td>CHILD'S PLAY</td>\n",
       "      <td>None</td>\n",
       "      <td>A Longfellow poem &amp; a Lillian Hellman play abo...</td>\n",
       "      <td>The Children's Hour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184710</th>\n",
       "      <td>2941</td>\n",
       "      <td>1997-05-19</td>\n",
       "      <td>Tiebreaker</td>\n",
       "      <td>THE AMERICAN REVOLUTION</td>\n",
       "      <td>None</td>\n",
       "      <td>On Nov. 15, 1777 Congress adopted this constit...</td>\n",
       "      <td>the Articles of Confederation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198973</th>\n",
       "      <td>4150</td>\n",
       "      <td>2002-09-20</td>\n",
       "      <td>Tiebreaker</td>\n",
       "      <td>LITERARY CHARACTERS</td>\n",
       "      <td>None</td>\n",
       "      <td>Hogwarts headmaster, he's considered by many t...</td>\n",
       "      <td>Professor Dumbledore</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        show_number   air_date       round                 category value  \\\n",
       "12305          5332 2007-11-13  Tiebreaker             CHILD'S PLAY  None   \n",
       "184710         2941 1997-05-19  Tiebreaker  THE AMERICAN REVOLUTION  None   \n",
       "198973         4150 2002-09-20  Tiebreaker      LITERARY CHARACTERS  None   \n",
       "\n",
       "                                                 question  \\\n",
       "12305   A Longfellow poem & a Lillian Hellman play abo...   \n",
       "184710  On Nov. 15, 1777 Congress adopted this constit...   \n",
       "198973  Hogwarts headmaster, he's considered by many t...   \n",
       "\n",
       "                               answer  \n",
       "12305             The Children's Hour  \n",
       "184710  the Articles of Confederation  \n",
       "198973           Professor Dumbledore  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How can I filter my dataframe to see the \n",
    "# questions asked in a Tiebreaker round?\n",
    "jeopardy[jeopardy['round'] == 'Tiebreaker']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean our text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"For the last 8 years of his life, Galileo was under house arrest for espousing this man's theory\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine zero-th question.\n",
    "jeopardy['question'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No. 2: 1912 Olympian; football star at Carlisle Indian School; 6 MLB seasons with the Reds, Giants & Braves'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine first question.\n",
    "jeopardy['question'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The city of Yuma in this state has a record average of 4,055 hours of sunshine each year'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine second question.\n",
    "jeopardy['question'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is particularly clean. But our \"toolbox\" for cleaning text data includes many things of which we may want to be aware:\n",
    "- Convert all words to lower case.\n",
    "- Tokenize text.\n",
    "- Remove HTML artifacts.\n",
    "- Remove punctuation.\n",
    "- Lemmatize/stem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert words to lower case.\n",
    "\n",
    "Python is case-sensitive (as are most languages!), but we often want to make apples-to-apples comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"For the last 8 years of his life, Galileo was under house arrest for espousing this man's theory\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine zero-th question.\n",
    "jeopardy['question'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on how we create columns of data, the `For` and `for` may be interpreted differently, even though these are the same word!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for the last 8 years of his life, galileo was under house arrest for espousing this man's theory\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print string in lower case.\n",
    "jeopardy['question'][0].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lower_case variable.\n",
    "lower_case = jeopardy['question'][0].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize text.\n",
    "\n",
    "When we **tokenize** text data, we split a string into smaller strings based on some pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['for',\n",
       " 'the',\n",
       " 'last',\n",
       " '8',\n",
       " 'years',\n",
       " 'of',\n",
       " 'his',\n",
       " 'life,',\n",
       " 'galileo',\n",
       " 'was',\n",
       " 'under',\n",
       " 'house',\n",
       " 'arrest',\n",
       " 'for',\n",
       " 'espousing',\n",
       " 'this',\n",
       " \"man's\",\n",
       " 'theory']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use .split() to tokenize our text data.\n",
    "lower_case.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokens.\n",
    "tokens = lower_case.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion 1**\n",
    "<details><summary>Why do you think tokenizing might be beneficial?</summary>\n",
    "\n",
    "- Tokenizing allows us to iterate through each individual word or phrase in our list. \n",
    "- We can then clean each token separately/individually.\n",
    "- We can also count up tokens meeting a certain condition, which enables us to better understand our data.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can use regular expressions to detect specific patterns, in case we want to do something more specific than just split items based one specific character string, like spaces.\n",
    "> The `nltk` library includes the `RegexpTokenizer()` function if you want to tokenize based on this specific pattern. Below, we'll see some examples of patterns we can identify!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] for\n",
      "[] the\n",
      "[] last\n",
      "['8'] 8\n",
      "[] years\n",
      "[] of\n",
      "[] his\n",
      "[] life,\n",
      "[] galileo\n",
      "[] was\n",
      "[] under\n",
      "[] house\n",
      "[] arrest\n",
      "[] for\n",
      "[] espousing\n",
      "[] this\n",
      "[] man's\n",
      "[] theory\n"
     ]
    }
   ],
   "source": [
    "# Use re.findall to find all tokens with a numeric digit.\n",
    "\n",
    "for token in tokens:\n",
    "    print(re.findall('\\d+', token), token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for'] for\n",
      "['the'] the\n",
      "['last'] last\n",
      "['8'] 8\n",
      "['years'] years\n",
      "['of'] of\n",
      "['his'] his\n",
      "['life', ','] life,\n",
      "['galileo'] galileo\n",
      "['was'] was\n",
      "['under'] under\n",
      "['house'] house\n",
      "['arrest'] arrest\n",
      "['for'] for\n",
      "['espousing'] espousing\n",
      "['this'] this\n",
      "['man', \"'s\"] man's\n",
      "['theory'] theory\n"
     ]
    }
   ],
   "source": [
    "# Use re.findall to split tokens up containing punctuation.\n",
    "\n",
    "for token in tokens:\n",
    "    print(re.findall('\\w+|\\$[\\d\\.]+|\\S+', token), token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] for\n",
      "[] the\n",
      "[] last\n",
      "[] 8\n",
      "[] years\n",
      "[] of\n",
      "[] his\n",
      "[] life,\n",
      "[] galileo\n",
      "[] was\n",
      "[] under\n",
      "[] house\n",
      "[] arrest\n",
      "[] for\n",
      "[] espousing\n",
      "[] this\n",
      "[] man's\n",
      "[] theory\n"
     ]
    }
   ],
   "source": [
    "# Use re.findall to select words beginning with a capital letter.\n",
    "# We shouldn't see any, since we've forced tokens to be lower case!\n",
    "\n",
    "for token in tokens:\n",
    "    print(re.findall('[A-Z]\\w+', token), token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['For'] For\n",
      "[] the\n",
      "[] last\n",
      "[] 8\n",
      "[] years\n",
      "[] of\n",
      "[] his\n",
      "[] life,\n",
      "['Galileo'] Galileo\n",
      "[] was\n",
      "[] under\n",
      "[] house\n",
      "[] arrest\n",
      "[] for\n",
      "[] espousing\n",
      "[] this\n",
      "[] man's\n",
      "[] theory\n"
     ]
    }
   ],
   "source": [
    "# Use re.findall to select words beginning with a capital letter.\n",
    "# If we look at the original jeopardy['question'][0].split()\n",
    "# before applying .lower(), then we should see something.\n",
    "\n",
    "for token in jeopardy['question'][0].split():\n",
    "    print(re.findall('[A-Z]\\w+', token), token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] for\n",
      "[] the\n",
      "[] last\n",
      "[] 8\n",
      "[] years\n",
      "[] of\n",
      "[] his\n",
      "[','] life,\n",
      "[] galileo\n",
      "[] was\n",
      "[] under\n",
      "[] house\n",
      "[] arrest\n",
      "[] for\n",
      "[] espousing\n",
      "[] this\n",
      "[\"'\"] man's\n",
      "[] theory\n"
     ]
    }
   ],
   "source": [
    "# Use re.findall to get non-letters and non-numbers.\n",
    "for token in tokens:\n",
    "    print(re.findall('[^a-zA-Z0-9]', token), token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove HTML artifacts.\n",
    "\n",
    "It is unlikely that any of these Jeopardy questions contain HTML artifacts that we'd want to discard, so we won't apply that to this data. \n",
    "\n",
    "However, lots of data will contain HTML! If you're scraping or downloading information, HTML code snippets like `<br>` or `\\` will likely show up. These will likely cause problems with your analysis -- your tokenizer (and your vectorizer, which we'll discuss later) may not understand what to do when `<br>` is encountered.\n",
    "\n",
    "> The `bs4` library includes a `BeautifulSoup` object and `.get_text()` method if you want to remove code snippets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatize/stem words.\n",
    "\n",
    "Some words may have similar meaning, but be spelled differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"For the last 8 years of his life, Galileo was under house arrest for espousing this man's theory\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine zero-th question.\n",
    "jeopardy['question'][0]\n",
    "\n",
    "# Year / years\n",
    "# Espouse / espousing\n",
    "# Theory / theories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to combine these words together, we might use a lemmatizer or stemmer. By \"combine,\" I mean, \"map all uses of the word `years` to `year`, map all uses of the word `espousing` to `espouse`, and so on.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The city of Yuma in this state has a record average of 4,055 hours of sunshine each year'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine second question.\n",
    "jeopardy['question'][2]\n",
    "\n",
    "# State (noun) / state (verb)\n",
    "# Average / mean\n",
    "# Record (adjective) / record (noun) / record (verb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizers and stemmers are not perfect. They're inexact!\n",
    "- Lemmatizers tend to be \"gentler.\" It changes fewer words, but may have more false negatives. (Words that should be changed, but aren't.)\n",
    "- Stemmers tend to be \"cruder.\" It changes more words, but may have more false positives. (Words that should not be changed, but are changed anyway.)\n",
    "\n",
    "> The `nltk` library contains lemmatizers and stemmers. I've used the `WordNetLemmatizer()` and the `PorterStemmer()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Put into one function and clean Jeopardy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jeopardy_clean(input_text):\n",
    "    # The input is a single string (one question), and \n",
    "    # the output is a single string (a cleaned question).\n",
    "    \n",
    "    # 1. Remove any punctuation.\n",
    "    letters_numbers = re.sub(\"[^a-zA-Z0-9]\", \" \", input_text)\n",
    "    \n",
    "    # 2. Convert to lower case. \n",
    "    lower_case = letters_numbers.lower()\n",
    "    \n",
    "    # 3. split into individual words.\n",
    "    words = lower_case.split()\n",
    "    \n",
    "    # If you wanted to add more in here, like:\n",
    "    # removing HTML artifacts,\n",
    "    # lemmatizing/stemming,\n",
    "    # removing stopwords,  <-- we'll use scikit-learn to do this later\n",
    "    # then you could do that here!\n",
    "    \n",
    "    # 4. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return(\" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning the Jeopardy questions...\n",
      "Review 10000 of 216930.\n",
      "Review 20000 of 216930.\n",
      "Review 30000 of 216930.\n",
      "Review 40000 of 216930.\n",
      "Review 50000 of 216930.\n",
      "Review 60000 of 216930.\n",
      "Review 70000 of 216930.\n",
      "Review 80000 of 216930.\n",
      "Review 90000 of 216930.\n",
      "Review 100000 of 216930.\n",
      "Review 110000 of 216930.\n",
      "Review 120000 of 216930.\n",
      "Review 130000 of 216930.\n",
      "Review 140000 of 216930.\n",
      "Review 150000 of 216930.\n",
      "Review 160000 of 216930.\n",
      "Review 170000 of 216930.\n",
      "Review 180000 of 216930.\n",
      "Review 190000 of 216930.\n",
      "Review 200000 of 216930.\n",
      "Review 210000 of 216930.\n",
      "Done! Cleaned all 216930 questions.\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list for our cleaned Jeopardy data.\n",
    "clean_data = []\n",
    "\n",
    "# How many questions do we have?\n",
    "total_questions = len(jeopardy['question'])\n",
    "\n",
    "print(\"Cleaning the Jeopardy questions...\")\n",
    "\n",
    "# Instantiate counter.\n",
    "count = 0\n",
    "\n",
    "# For every question in our data...\n",
    "for question in jeopardy['question']:\n",
    "    \n",
    "    # Clean question, then append to clean_data.\n",
    "    clean_data.append(jeopardy_clean(question))\n",
    "    \n",
    "    # If the index is divisible by 10,000, print a message.\n",
    "    if (count + 1) % 10_000 == 0:\n",
    "        print(f'Review {count + 1} of {total_questions}.')\n",
    "    \n",
    "    count += 1\n",
    "\n",
    "# Let us know when we're done and how many questions are cleaned.\n",
    "print(f'Done! Cleaned all {total_questions} questions.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['for the last 8 years of his life galileo was under house arrest for espousing this man s theory',\n",
       " 'no 2 1912 olympian football star at carlisle indian school 6 mlb seasons with the reds giants braves',\n",
       " 'the city of yuma in this state has a record average of 4 055 hours of sunshine each year',\n",
       " 'in 1963 live on the art linkletter show this company served its billionth burger',\n",
       " 'signer of the dec of indep framer of the constitution of mass second president of the united states',\n",
       " 'in the title of an aesop fable this insect shared billing with a grasshopper',\n",
       " 'built in 312 b c to link rome the south of italy it s still in use today',\n",
       " 'no 8 30 steals for the birmingham barons 2 306 steals for the bulls',\n",
       " 'in the winter of 1971 72 a record 1 122 inches of snow fell at rainier paradise ranger station in this state',\n",
       " 'this housewares store was named for the packaging its merchandise came in was first displayed on']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine first ten questions.\n",
    "clean_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize our text data.\n",
    "\n",
    "We have our cleaned data! Now we can start preparing our model.\n",
    "\n",
    "In machine learning:\n",
    "- `X` is a matrix/dataframe of real numbers.\n",
    "- `y` is a vector/series of real numbers.\n",
    "\n",
    "Our $Y$ variable will be, \"Was this question asked during the 'Double Jeopardy!' round?\"\n",
    "- Often, we might want a more _interesting_ $Y$ variable, like predicting whether or not the clue is from the `Before and After` category. However, it's tough to define categories that don't fall prey to [unbalanced/imbalanced classes](https://blog.roboflow.com/handling-unbalanced-classes/), which makes machine learning more difficult. Today, we're going to pick a slightly less interesting but easier problem to solve so that we can focus our attention on natural language processing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our y variable.\n",
    "y = pd.Series([1 if item == 'Double Jeopardy!' else 0 for item in jeopardy['round']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    111018\n",
       "1    105912\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm we created our variable correctly.\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our X variable.\n",
    "X = pd.DataFrame(clean_data, columns=['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into the training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X['question'],\n",
    "                                                    y,\n",
    "                                                    test_size=0.2, # 80% training / 20% test.\n",
    "                                                    stratify=y,    # keep distribution of Y same across train/test\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text data (natural language data) is often not already organized as a matrix or vector of real numbers. For example, above, we have a list of cleaned questions.\n",
    "\n",
    "**Vectorizing our text data** describes one fairly simple process for converting a set of text data into a matrix of real numbers.\n",
    "\n",
    "There are two basic, commonly used types of vectorizer: `CountVectorizer` and `TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer\n",
    "\n",
    "<img src=\"./images/countvectorizer.png\" alt=\"drawing\" width=\"700\"/>\n",
    "\n",
    "[Source.](https://towardsdatascience.com/nlp-learning-series-part-2-conventional-methods-for-text-classification-40f2839dd061)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate CountVectorizer.\n",
    "\n",
    "cvec = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the vectorizer on our corpus. (corpus = set of words)\n",
    "\n",
    "cvec.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the corpus.\n",
    "X_train = cvec.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000</th>\n",
       "      <th>0003</th>\n",
       "      <th>000529</th>\n",
       "      <th>000th</th>\n",
       "      <th>001</th>\n",
       "      <th>002</th>\n",
       "      <th>0025</th>\n",
       "      <th>004</th>\n",
       "      <th>...</th>\n",
       "      <th>zygomatic</th>\n",
       "      <th>zygote</th>\n",
       "      <th>zygotes</th>\n",
       "      <th>zymase</th>\n",
       "      <th>zynga</th>\n",
       "      <th>zyplast</th>\n",
       "      <th>zyuganov</th>\n",
       "      <th>zyzzyx</th>\n",
       "      <th>zz</th>\n",
       "      <th>zzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173539</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173540</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173541</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173542</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173543</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>173544 rows Ã— 79174 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        00  000  0000  0003  000529  000th  001  002  0025  004  ...  \\\n",
       "0        0    0     0     0       0      0    0    0     0    0  ...   \n",
       "1        0    0     0     0       0      0    0    0     0    0  ...   \n",
       "2        0    0     0     0       0      0    0    0     0    0  ...   \n",
       "3        0    0     0     0       0      0    0    0     0    0  ...   \n",
       "4        0    0     0     0       0      0    0    0     0    0  ...   \n",
       "...     ..  ...   ...   ...     ...    ...  ...  ...   ...  ...  ...   \n",
       "173539   0    0     0     0       0      0    0    0     0    0  ...   \n",
       "173540   0    0     0     0       0      0    0    0     0    0  ...   \n",
       "173541   0    0     0     0       0      0    0    0     0    0  ...   \n",
       "173542   0    0     0     0       0      0    0    0     0    0  ...   \n",
       "173543   0    0     0     0       0      0    0    0     0    0  ...   \n",
       "\n",
       "        zygomatic  zygote  zygotes  zymase  zynga  zyplast  zyuganov  zyzzyx  \\\n",
       "0               0       0        0       0      0        0         0       0   \n",
       "1               0       0        0       0      0        0         0       0   \n",
       "2               0       0        0       0      0        0         0       0   \n",
       "3               0       0        0       0      0        0         0       0   \n",
       "4               0       0        0       0      0        0         0       0   \n",
       "...           ...     ...      ...     ...    ...      ...       ...     ...   \n",
       "173539          0       0        0       0      0        0         0       0   \n",
       "173540          0       0        0       0      0        0         0       0   \n",
       "173541          0       0        0       0      0        0         0       0   \n",
       "173542          0       0        0       0      0        0         0       0   \n",
       "173543          0       0        0       0      0        0         0       0   \n",
       "\n",
       "        zz  zzzz  \n",
       "0        0     0  \n",
       "1        0     0  \n",
       "2        0     0  \n",
       "3        0     0  \n",
       "4        0     0  \n",
       "...     ..   ...  \n",
       "173539   0     0  \n",
       "173540   0     0  \n",
       "173541   0     0  \n",
       "173542   0     0  \n",
       "173543   0     0  \n",
       "\n",
       "[173544 rows x 79174 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert X_train into a DataFrame.\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train.toarray(),\n",
    "                          columns=cvec.get_feature_names())\n",
    "X_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform test data.\n",
    "X_test = cvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our tokens are now stored as a bag-of-words. This is a simplified way of looking at and storing our data.\n",
    "- Bag-of-words representations discard grammar, order, and structure in the text but track occurrences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion 2**\n",
    "<details><summary>What might be some of the advantages of using this bag-of-words approach when modeling?</summary>\n",
    "\n",
    "- Efficient to store.\n",
    "- Efficient to model.\n",
    "- Keeps a decent amount of information.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion 3**\n",
    "<details><summary>What might be some of the disadvantages of using this bag-of-words approach when modeling?</summary>\n",
    "\n",
    "- Since bag-of-words models discard grammar, order, structure, and context, we lose a decent amount of information.\n",
    "- Phrases like \"not bad\" or \"not good\" won't be interpreted properly.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate logistic regression model.\n",
    "\n",
    "lr = LogisticRegression(solver = 'liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(solver='liblinear')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit logistic regression model.\n",
    "\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy on Training Data: 0.7641001705619325\n"
     ]
    }
   ],
   "source": [
    "# Get model accuracy on training data.\n",
    "\n",
    "print(f'Model Accuracy on Training Data: {lr.score(X_train, y_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy on Testing Data: 0.5725579680081132\n"
     ]
    }
   ],
   "source": [
    "# Get model accuracy on testing data.\n",
    "\n",
    "print(f'Model Accuracy on Testing Data: {lr.score(X_test, y_test)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TfidfVectorizer\n",
    "\n",
    "<img src=\"./images/tfidfvectorizer.png\" alt=\"drawing\" width=\"900\"/>\n",
    "\n",
    "[Source.](https://towardsdatascience.com/nlp-learning-series-part-2-conventional-methods-for-text-classification-40f2839dd061)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into the training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X['question'],\n",
    "                                                    y,\n",
    "                                                    test_size=0.2, # 80% training / 20% test\n",
    "                                                    stratify=y,    # keep distribution of Y same across train/test\n",
    "                                                    random_state=42)\n",
    "\n",
    "# Instantiate TfidfVectorizer.\n",
    "tvec = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer and transform the training data.\n",
    "X_train = tvec.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data.\n",
    "X_test = tvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy on Training Data: 0.7151097128105841\n",
      "Model Accuracy on Training Data: 0.584335960909049\n"
     ]
    }
   ],
   "source": [
    "# Instantiate logistic regression model.\n",
    "lr = LogisticRegression(solver = 'liblinear')\n",
    "\n",
    "# Fit logistic regression model.\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Get model accuracy on training data.\n",
    "print(f'Model Accuracy on Training Data: {lr.score(X_train, y_train)}')\n",
    "\n",
    "# Get model accuracy on testing data.\n",
    "print(f'Model Accuracy on Training Data: {lr.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "---\n",
    "\n",
    "**Discussion 4**\n",
    "<details><summary>What is a hyperparameter?</summary>\n",
    "\n",
    "- A hyperparameter is a built-in option that affects our model, but our model cannot learn these from our data!\n",
    "- Examples of hyperparameters include:\n",
    "    - the value of $k$ and the distance metric in $k$-nearest neighbors,\n",
    "    - our regularization constants $\\alpha$ or $C$ in linear and logistic regression.\n",
    "</details>\n",
    "\n",
    "There are many different hyperparameters of `CountVectorizer` that can affect the fit of our model!\n",
    "- `stop_words`\n",
    "- `max_features`, `max_df`, `min_df`\n",
    "- `ngram_range`\n",
    "\n",
    "\n",
    "#### `stop_words`\n",
    "\n",
    "Some words are so common that they may not provide legitimate information about the $Y$ variable we're trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'behind', 'via', 'rather', 'it', 'last', 'third', 'sixty', 'was', 'nine', 'myself', 'in', 'than', 'formerly', 'least', 'someone', 'our', 'twelve', 'part', 'otherwise', 'may', 'made', 'along', 'everything', 'must', 'eleven', 'top', 'always', 'became', 'others', 'nevertheless', 'am', 'yourself', 'sincere', 'first', 'whatever', 'mill', 'hundred', 'former', 'thus', 'while', 'whereupon', 'none', 'twenty', 'interest', 'keep', 'latter', 'five', 'go', 'can', 'even', 'herself', 'seeming', 'whose', 'whither', 'several', 'some', 'somewhere', 'cant', 'forty', 'however', 'here', 'together', 'call', 're', 'nobody', 'so', 'yours', 'bottom', 'by', 'see', 'through', 'side', 'every', 'beyond', 'perhaps', 'himself', 'ltd', 'whole', 'hence', 'any', 'inc', 'namely', 'thereupon', 'eg', 'per', 'fifty', 'too', 'what', 'either', 'much', 'that', 'de', 'themselves', 'these', 'ourselves', 'con', 'i', 'somehow', 'well', 'co', 'such', 'only', 'above', 'get', 'if', 'they', 'without', 'cry', 'about', 'amongst', 'and', 'whereafter', 'move', 'becomes', 'full', 'is', 'hereby', 'around', 'therefore', 'thru', 'off', 'ten', 'whom', 'might', 'mine', 'all', 'hereafter', 'few', 'over', 'since', 'one', 'she', 'find', 'anything', 'as', 'cannot', 'due', 'further', 'thin', 'yourselves', 'before', 'would', 'a', 'amount', 'sometime', 'name', 'most', 'now', 'seem', 'six', 'out', 'become', 'to', 'not', 'except', 'whereby', 'where', 'almost', 'or', 'whereas', 'still', 'never', 'often', 'besides', 'system', 'thick', 'no', 'already', 'also', 'below', 'fire', 'has', 'describe', 'at', 'against', 'me', 'when', 'up', 'have', 'him', 'why', 'towards', 'eight', 'everyone', 'fill', 'throughout', 'could', 'his', 'on', 'wherein', 'front', 'do', 'though', 'being', 'been', 'seems', 'who', 'two', 'wherever', 'etc', 'its', 'something', 'whether', 'onto', 'show', 'under', 'after', 'afterwards', 'less', 'with', 'down', 'we', 'upon', 'therein', 'put', 'an', 'indeed', 'of', 'anyone', 'empty', 'everywhere', 'becoming', 'how', 'many', 'toward', 'very', 'thereby', 'whenever', 'mostly', 'un', 'them', 'anywhere', 'same', 'couldnt', 'noone', 'thereafter', 'meanwhile', 'elsewhere', 'anyhow', 'own', 'their', 'serious', 'ie', 'but', 'my', 'another', 'anyway', 'latterly', 'among', 'from', 'found', 'again', 'ever', 'nothing', 'were', 'for', 'three', 'whoever', 'there', 'will', 'please', 'back', 'whence', 'this', 'into', 'between', 'you', 'those', 'herein', 'more', 'four', 'until', 'although', 'during', 'beside', 'are', 'nowhere', 'should', 'moreover', 'which', 'us', 'her', 'across', 'enough', 'within', 'be', 'itself', 'hers', 'fifteen', 'else', 'ours', 'amoungst', 'detail', 'next', 'your', 'nor', 'beforehand', 'hasnt', 'he', 'other', 'done', 'had', 'then', 'sometimes', 'hereupon', 'give', 'neither', 'bill', 'yet', 'thence', 'both', 'the', 'alone', 'once', 'seemed', 'take', 'because', 'each'})\n"
     ]
    }
   ],
   "source": [
    "# Let's look at sklearn's stopwords.\n",
    "print(CountVectorizer(stop_words = 'english').get_stop_words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CountVectorizer` gives you the option to eliminate stopwords from your corpus when instantiating your vectorizer.\n",
    "\n",
    "```python\n",
    "cvec = CountVectorizer(stop_words='english')\n",
    "```\n",
    "\n",
    "You can optionally pass your own list of stopwords that you'd like to remove.\n",
    "```python\n",
    "cvec = CountVectorizer(stop_words=['list', 'of', 'words', 'to', 'stop'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocabulary size\n",
    "\n",
    "---\n",
    "One downside to `CountVectorizer` and `TfidfVectorizer` is the size of its vocabulary (`cvec.get_feature_names()`) can get really large. We're creating one column for every unique token in your corpus of data!\n",
    "\n",
    "There are three hyperparameters to help you control this.\n",
    "\n",
    "1. You can set `max_features` to only include the $N$ most popular vocabulary words in the corpus.\n",
    "\n",
    "```python\n",
    "cvec = CountVectorizer(max_features=1_000) # Only the top 1,000 words from the entire corpus will be saved\n",
    "```\n",
    "\n",
    "2. You can tell `CountVectorizer` to only consider words that occur in **at least** some number of documents.\n",
    "\n",
    "```python\n",
    "cvec = CountVectorizer(min_df=2) # A word must occur in at least two documents from the corpus\n",
    "```\n",
    "\n",
    "3. Conversely, you can tell `CountVectorizer` to only consider words that occur in **at most** some percentage of documents.\n",
    "\n",
    "```python\n",
    "cvec = CountVectorizer(max_df=.98) # Ignore words that occur in > 98% of the documents from the corpus\n",
    "```\n",
    "\n",
    "Both `max_df` and `min_df` can accept either an integer or a float.\n",
    "- An integer tells us the number of documents.\n",
    "- A float tells us the percentage of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion 5**\n",
    "<details><summary>Why might we want to control these vocabulary size hyperparameters?</summary>\n",
    "    \n",
    "- If we have too many features, our models may take a **very** long time to fit.\n",
    "- Control for overfitting/underfitting.\n",
    "- Words in 99% of documents or words occuring in only one document might not be very informative.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `ngram_range`\n",
    "\n",
    "---\n",
    "\n",
    "`CountVectorizer` has the ability to capture $n$-word phrases, also called $n$-grams. Consider the following:\n",
    "\n",
    "> The quick brown fox jumps over the lazy dog.\n",
    "\n",
    "In the example sentence, the 2-grams are:\n",
    "- 'The quick'\n",
    "- 'quick brown'\n",
    "- 'brown fox'\n",
    "- 'fox jumps'\n",
    "- 'jumps over'\n",
    "- 'over the'\n",
    "- 'the lazy'\n",
    "- 'lazy dog.'\n",
    "\n",
    "The `ngram_range` determines what $n$-grams should be considered as features.\n",
    "\n",
    "```python\n",
    "cvec = CountVectorizer(ngram_range=(1,2)) # Captures every 1-gram and every 2-gram\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion 6**\n",
    "<details><summary>How many 3-grams would be generated from the phrase \"the quick brown fox jumped over the lazy dog?\"</summary>\n",
    "\n",
    "- Seven 3-grams.\n",
    "    - 'The quick brown'\n",
    "    - 'quick brown fox'\n",
    "    - 'brown fox jumped'\n",
    "    - 'fox jumped over'\n",
    "    - 'jumped over the'\n",
    "    - 'over the lazy'\n",
    "    - 'the lazy dog.'\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion 7**\n",
    "<details><summary>Why might we want to change ngram_range to something other than (1,1)?</summary>\n",
    "\n",
    "- We can work with multi-word phrases like \"not good\" or \"very hot.\"\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "---\n",
    "\n",
    "We may want to test lots of different values of hyperparameters in our `CountVectorizer` and `TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X['question'],\n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion 8**\n",
    "<details><summary>What is GridSearch?</summary>\n",
    "    \n",
    "- GridSearch allows us to try different values of different hyperparameters, measure our model's performance on each one, and return the best model.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set a pipeline up with two stages:\n",
    "# 1. CountVectorizer (transformer)\n",
    "# 2. LogisticRegression (estimator)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('lr', LogisticRegression(solver = 'liblinear'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **pipeline** allows us to GridSearch over both:\n",
    "- one or more transformers (like our CountVectorizer)\n",
    "- an estimator (a model, like our logistic regression model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search over the following values of hyperparameters:\n",
    "# Maximum number of features fit: 1,000 and 2,000.\n",
    "# Check (individual tokens) and also check (individual tokens and 2-grams).\n",
    "# Check removing English stopwords and not removing any stopwords.\n",
    "\n",
    "pipe_params = {\n",
    "    'cvec__max_features': [1_000, 2_000],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': ['english', None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate GridSearchCV.\n",
    "\n",
    "gs_cvec = GridSearchCV(pipe, # what object are we optimizing?\n",
    "                       param_grid=pipe_params, # what parameters values are we searching?\n",
    "                       cv=5) # 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion 9**\n",
    "<details><summary>How many models are we fitting here?</summary>\n",
    "\n",
    "- 2 max_features\n",
    "- 2 ngram_range\n",
    "- 2 stop_words\n",
    "- 5-fold CV\n",
    "- 2 * 2 * 2 * 5 = 40 models\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248.04471611976624\n"
     ]
    }
   ],
   "source": [
    "# Start stopwatch.\n",
    "t0 = time.time()\n",
    "\n",
    "# Fit GridSearch to training data.\n",
    "gs_cvec.fit(X_train, y_train)\n",
    "\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6033340248006269"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate accuracy on training data.\n",
    "gs_cvec.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5849352325634998"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate accuracy on testing data.\n",
    "gs_cvec.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_features': 2000,\n",
       " 'cvec__ngram_range': (1, 1),\n",
       " 'cvec__stop_words': None}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What model performed best?\n",
    "gs_cvec.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All at once, now, with `TfidfVectorizer`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy on Training Data: 0.6042444567371963.\n",
      "Model Accuracy on Testing data: 0.5866869497072789.\n",
      "Best parameter values: {'tvec__max_features': 2000, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': None}\n"
     ]
    }
   ],
   "source": [
    "# Redefine training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X['question'],\n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)\n",
    "\n",
    "# Let's set a pipeline up with two stages:\n",
    "# 1. TfidfVectorizer (transformer)\n",
    "# 2. LogisticRegression (estimator)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('lr', LogisticRegression(solver = 'liblinear'))\n",
    "])\n",
    "\n",
    "# Search over the following values of hyperparameters:\n",
    "# Maximum number of features fit: 1,000 and 2,000.\n",
    "# Check (individual tokens) and also check (individual tokens and 2-grams).\n",
    "# Check removing English stopwords and not removing any stopwords.\n",
    "\n",
    "pipe_params = {\n",
    "    'tvec__max_features': [1_000, 2_000],\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': ['english', None]\n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV.\n",
    "gs_tvec = GridSearchCV(pipe, # what object are we optimizing?\n",
    "                       param_grid=pipe_params, # what parameters values are we searching?\n",
    "                       cv=5) # 5-fold cross-validation.\n",
    "\n",
    "\n",
    "# Fit GridSearch to training data.\n",
    "gs_tvec.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate accuracy on training data.\n",
    "print(f'Model Accuracy on Training Data: {gs_tvec.score(X_train, y_train)}.')\n",
    "\n",
    "# Evaluate accuracy on testing data.\n",
    "print(f'Model Accuracy on Testing data: {gs_tvec.score(X_test, y_test)}.')\n",
    "\n",
    "# The logistic regression model with what\n",
    "# hyperparameters performed best? (As measured\n",
    "# by accuracy.)\n",
    "print(f'Best parameter values: {gs_tvec.best_params_}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
